{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a new BoM from 3dx files\n",
    "\n",
    "This is reading in extracts produced by Alec, adding the function group and system/subsystem, and doing the roll up quantity calcs in the absence of 3DX doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import xlwings as xw\n",
    "import glob\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(download_dir):\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    past = time.time() - 2*60*60 # 2 hours\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            # was using this to filter what filenames to find\n",
    "            if 'ENO' in fn:            \n",
    "                filepath = os.path.join(p, fn)\n",
    "                if os.path.getmtime(filepath) >= past:\n",
    "                    files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file):\n",
    "    # get 3dx extract\n",
    "    print (\"opening {} at: {}\".format(file, time.strftime(\"%H:%M:%S\", time.localtime())))\n",
    "    with open(file, \"rb\") as f:\n",
    "        try:\n",
    "\n",
    "            if 'csv' in file:\n",
    "                BOM = pd.DataFrame()\n",
    "                BOM = pd.read_csv(f, low_memory=False) \n",
    "                # skip first n rows that are header information out of 3DX\n",
    "                if 'Level' not in BOM.columns:\n",
    "                    print (\"didn't get the column headers so finding them myself\")            \n",
    "                    n = BOM[BOM.iloc[:, 0] == 'Level'].index.values[0]\n",
    "                    # create the column headers\n",
    "                    BOM.columns = BOM.iloc[n]\n",
    "                    # drop the top rows above the header row we found\n",
    "                    BOM = BOM[n+1:]\n",
    "\n",
    "            if 'xls' in file:\n",
    "                # BOM = pd.read_csv(f, low_memory=False, skiprows=8) \n",
    "                BOM = pd.read_excel(f)             \n",
    "            # sheetnames = [sheet for sheet in f.sheet_names]\n",
    "        except Exception as e:\n",
    "            print (\"{}\".format(e))\n",
    "\n",
    "    return BOM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_function_group(df):\n",
    "    # Add Function and Sub Group if it doesn't already exist\n",
    "\n",
    "    # - Level 0 = Model Variant   \n",
    "    # - Level 1 = Function Group Area   \n",
    "    # - Level 2 = System   \n",
    "    # - Level 3 = Sub Systems\n",
    "    # - level 4 = AMs/SAs??\n",
    "\n",
    "    # Find each one and forward fill to the next occurrence\n",
    "    # function group - level 1\n",
    "    df['Function Group'] = np.where(df['Level'].isin([0,1]), df['Description'], None)\n",
    "    df['Function Group'] = df['Function Group'].ffill()\n",
    "\n",
    "    # System - level 2\n",
    "    df['System'] = np.where(df['Level'] == 2, df['Description'], None)\n",
    "    df['System'] = np.where(df['Level'] >= 2, df['System'].ffill(), None)\n",
    "\n",
    "    # SUB_System = level 3\n",
    "    df['Sub System'] = np.where(df['Level'] == 3, df['Description'], None)\n",
    "    df['Sub System'] = np.where(df['Level'] >= 3, df['Sub System'].ffill(), df['Sub System'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index':'orig_sort'}, inplace=True)\n",
    "\n",
    "    try:\n",
    "        df.rename(columns={'Title (Instance)':'Instance Title'}, inplace=True)\n",
    "    except ValueError:\n",
    "        print (\"didn't find Title (Instance) to rename\")\n",
    "\n",
    "    try:\n",
    "        df.rename(columns={'Occurrences':'Quantity'}, inplace=True)\n",
    "    except ValueError:\n",
    "        print (\"didn't find Occurrences to rename\")\n",
    "\n",
    "    # replace any Mass (kg) columns with Mass\n",
    "    df.columns = df.columns.str.replace('Mass (kg)', 'Mass')\n",
    "\n",
    "\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smartsheet_cols(df, extract_date):\n",
    "    # matching key now dealt with when creating the parent part\n",
    "    # df['Matching Key'] = np.where(df['GParent Part'].isna(), None, df['Parent Part'].astype(str) + df['GParent Part'].astype(str))\n",
    "    # df['Matching Key'] = np.where(df['Matching Key'].isna(), df['Title'].astype(str), df['Title'].astype(str) + df['Matching Key'].astype(str))\n",
    "    # df['Matching Key'] = np.where(df['Parent Part'].isna(), df['Title'].astype(str), df[\"Title\"].astype(str) + df[\"Parent Part\"].astype(str))\n",
    "    # force matching key to always be upper case\n",
    "    df['Matching Key'] = df['Matching Key'].str.upper()\n",
    "    # need to correct the percent missing for matching key column\n",
    "    df.loc['percent_missing','Matching Key'] = 0\n",
    "    df['Last Export Date'] = extract_date\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mandatory_attributes(bom_cols):\n",
    "    # read in mandatory fields from file in same directory\n",
    "    with open('mandatory_attributes.ini', 'r') as f:\n",
    "        lst = f.readlines()\n",
    "\n",
    "    mand_cols = [line.rstrip() for line in lst]\n",
    "\n",
    "    missing_cols = list(set(mand_cols) - set(bom_cols))\n",
    "\n",
    "    if len(missing_cols) > 0:\n",
    "        print (\"missing mandatory attributes: {}\".format(missing_cols))\n",
    "        sys.exit()\n",
    "\n",
    "    return mand_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_columns(df):\n",
    "\n",
    "    cols_to_order = ['BOM COUNT',\n",
    "        'Matching Key',\n",
    "        'Last Export Date',\n",
    "        'orig_sort',\n",
    "        'Function Group',\n",
    "        'System',\n",
    "        'Sub System',\n",
    "        'Level',\n",
    "        'Title',\n",
    "        'Parent Part',\n",
    "        'Revision',\n",
    "        'Description',\n",
    "        'Quantity',\n",
    "        'Source Code',\n",
    "        'UOM',\n",
    "        'Provide',\n",
    "        'Actual Mass',\n",
    "        'CAD Mass',\n",
    "        'CAD Material',\n",
    "        'Programme Maturity']\n",
    "        # 'Subtype']\n",
    "    \n",
    "    # # write out mandatory attributes to local file.  Not reading in from here as there is always some code change required anyway!\n",
    "    # with open('mandatory_attributes.ini', 'w') as f:\n",
    "    #     for col in cols_to_order:\n",
    "    #         # ignore the internal cols I create\n",
    "    #         if col not in (['Matching Key','Last Export Date','orig_sort','Parent Part']):\n",
    "    #             f.write('{}\\n'.format(col))\n",
    "\n",
    "    try:\n",
    "        ordered_cols = cols_to_order + (df.columns.sort_values().drop(cols_to_order).tolist())\n",
    "        df = df[ordered_cols]\n",
    "    except KeyError as e:\n",
    "        raise Exception (\"Missing an expected column in the extract: {}\".format(e))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sa_index(df):\n",
    "    # fill level 3s with orig_sort\n",
    "    # ffill everything with the level 3 orig_sort\n",
    "    # fill level 4s with the level 3 orig_sort + its own orig_sort\n",
    "    # fill < level 3 with own orig_sort\n",
    "    # NaN > level 4 and refill with the sa_index from level 4 above\n",
    "    df['SA_Index'] = np.where(df['Level'] == 3, df['orig_sort'].astype(str), np.nan)\n",
    "    df['SA_Index'] = df['SA_Index'].ffill()\n",
    "    df['SA_Index'] = np.where(df['Level'] == 4, df['SA_Index'] + '_' + df['orig_sort'].astype(str), df['SA_Index'])\n",
    "    # forward fill so that > Level 5 get the same index\n",
    "    df['SA_Index'] = np.where(df['Level'] < 3, df['orig_sort'].astype(str), df['SA_Index'])\n",
    "    df['SA_Index'] = np.where(df['Level'] > 4, np.nan, df['SA_Index'])\n",
    "    df['SA_Index'] = df['SA_Index'].ffill()\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sa_index2(df):\n",
    "    df['SA_Index'] = np.where(df['Assembly'], df['orig_sort'].astype(str), np.nan)\n",
    "    df['SA_Index'] = df['SA_Index'].ffill()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quantities(df):\n",
    "\n",
    "    # if the extract hasn't come with quantity column then we need to calculate our own grouping like parts together\n",
    "    # this is only being done for level 4 and below - level 4 is assumed to be assembly level\n",
    "    # there is a problem with this, though, as we don't know what level the extract was done at\n",
    "    \n",
    "    # groupby:\n",
    "    # SA_Index - created earlier, this will group the parts within an assembly (level 4 and below) - is this correct??\n",
    "    # Title - this is the part number\n",
    "    # Parent Part - created earlier, this will group only parts at the same level with the same parent\n",
    "    # Level - this is probably not required if we are using parent part, but ensures we group at the same level\n",
    "\n",
    "    # groupby Title and sum (size).  Save as a new df called qty\n",
    "    qty = BOM_pp.groupby(['SA_Index','Title','Parent Part','Level'], dropna=False).size().reset_index(name='Quantity')\n",
    "    # qty = BOM_pp.groupby(['Title','Parent Part','Level'], dropna=False).size().reset_index(name='Quantity')\n",
    "\n",
    "    # merge qty with BOM on SA_Index to get all the other columns back\n",
    "    qty2 = pd.merge(qty, BOM_pp, on=['SA_Index','Title','Parent Part','Level'])\n",
    "    # qty2 = pd.merge(qty, BOM_pp, on=['Title','Parent Part','Level'])\n",
    "    # qty2 = pd.concat([qty, BOM_pp])\n",
    "\n",
    "    # need to drop dups using only a subset of cols, creating new_bom df\n",
    "    new_bom = qty2.drop_duplicates(subset=['SA_Index','Title','Parent Part', 'Level'])\n",
    "    # new_bom = qty2.drop_duplicates(subset=['Title','Parent Part', 'Level'])\n",
    "    # sort the new_nom df by the orig_sort field to make sure it's the order it came out of 3dx\n",
    "    new_bom = new_bom.sort_values(by='orig_sort')\n",
    "    \n",
    "    # don't think there are any names to rename?\n",
    "    new_bom.rename(columns={\n",
    "        'Title_y':'Title',\n",
    "        'Parent Part_x':'Parent Part',\n",
    "        'Quantity_x':'Quantity',\n",
    "        'Level_x':'Level'\n",
    "    }, inplace=True)    \n",
    "\n",
    "    return new_bom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_part(df):\n",
    "    # reset index before trying to update, otherwise multiple rows get updated\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df['Parent Part'] = None\n",
    "\n",
    "    level = {}\n",
    "    previous_parent_part=0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        current_part_number = row['Title']\n",
    "        current_part_level = row['Level']\n",
    "\n",
    "        # write part number to dictionary under current part level\n",
    "        level[current_part_level] = current_part_number\n",
    "\n",
    "        # reset higher levels for each assembly\n",
    "        # remove entries from higher levels\n",
    "        keys = [k for k in level if k > current_part_level]\n",
    "        for x in keys:\n",
    "            del level[x]\n",
    "\n",
    "        if current_part_level > 0:\n",
    "            # get the max part level from the level dictionary that's less than current part level\n",
    "            previous_parent_level = max(k for k in level if k < current_part_level)\n",
    "\n",
    "            # update the parent part\n",
    "            # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "            df.at[i,'Parent Part'] = level[previous_parent_level]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gparent_part(df):\n",
    "    # reset index before trying to update, otherwise multiple rows get updated\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df['Parent Part'] = None\n",
    "    df['Matching Key'] = None\n",
    "\n",
    "    level = {}\n",
    "    previous_part_level=0\n",
    "    gparent_part_level=0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        current_part_number = row['Title']\n",
    "        current_part_level = row['Level']\n",
    "\n",
    "        # write part number to dictionary under current part level\n",
    "        level[current_part_level] = current_part_number\n",
    "\n",
    "        # reset higher levels for each assembly\n",
    "        # remove entries from higher levels\n",
    "        keys = [k for k in level if k > current_part_level]\n",
    "        for x in keys:\n",
    "            del level[x]\n",
    "\n",
    "        if current_part_level > 0:\n",
    "            # get the max part level from the level dictionary that's less than current part level\n",
    "            previous_part_level = max(k for k in level if k < current_part_level)\n",
    "\n",
    "            # update the parent part\n",
    "            # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "            df.at[i,'Parent Part'] = level[previous_part_level]\n",
    "        \n",
    "        # if previous_part_level > 0:\n",
    "            # get the max part level from the level dictionary that's less than previous parent level\n",
    "            # gparent_part_level = max(k for k in level if k < previous_part_level)\n",
    "            # gparent_part_level = ''.join((level.values()))\n",
    "\n",
    "\n",
    "            # update the parent part\n",
    "            # print (i, \"Parent part {} from previous level {}\".format(level[previous_parent_level], previous_parent_level))\n",
    "        df.at[i,'Matching Key'] = ''.join((level.values()))\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xml(df):\n",
    "    BOM_xml = df\n",
    "    # get rid of spaces, slashes and chars xml can't handle\n",
    "    BOM_xml.columns = df.columns.str.replace(' ', '_')\n",
    "    BOM_xml.columns = df.columns.str.replace('/', '_')\n",
    "    BOM_xml.columns = df.columns.str.replace('&', '')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shuttle_folder(product):\n",
    "    # After update of BoM names\n",
    "    # •\tXP 5 door master product - T48e-01-Z00001 \n",
    "    # •\tXP 3 door master product - T48e-02-Z00001\n",
    "    # •\tVP 5 door master product – T48e-01-Z00005\n",
    "    # •\tVP 3 door master product - T48e-02-Z00005\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config_file = 'product_structures_config.ini'\n",
    "    config.read(config_file)\n",
    "\n",
    "    # read the Product_Structures section and look up the product\n",
    "    try:\n",
    "        ds_folder = config['Product_Structures'][product]\n",
    "    except KeyError:\n",
    "        print (\"No entry in file {}, section: Product_Structures, for key {}.  Using 'default'\".format(config_file, product))\n",
    "        ds_folder = 'default'\n",
    "\n",
    "    return ds_folder\n",
    "    \n",
    "\n",
    "    # if 'T48e-01-Z00001' in product:\n",
    "    #     return 'VP 5 door master product - T48e-01-Z00001'\n",
    "    # elif 'T48e-02-Z00001' in product:\n",
    "    #     return 'XP 3 door master product - T48e-02-Z00001'\n",
    "    # elif 'T48e-01-Z00005' in product:\n",
    "    #     return 'XP 5 door master product – T48e-01-Z00005'\n",
    "    # elif 'T48e-02-Z00005' in product:\n",
    "    #     return 'VP 3 door master product - T48e-02-Z00005'\n",
    "    # elif 'T48e-M1-Z00001' in product:\n",
    "    #     return 'Chassis Mule - T48e-M1-Z00001'\n",
    "    # elif 'T48e-01-Z00003' in product:\n",
    "    #     return 'Kinematics and compliance - T48e-01-Z00003'\n",
    "    # elif 'T53-Z00001' in product:\n",
    "    #     return 'T53 - Z00001 - BOM Export'\n",
    "    # elif 'T53-Z00002' in product:\n",
    "    #     return 'T53 - Z00002 - BOM Export'\n",
    "    # else:\n",
    "    #     return 'default'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COG_split(df):\n",
    "    # split the COG field (where it is populated) into COG x, COG y, COG z \n",
    "    # need to make sure COG is a string field - if is it completely empty, all nan will make it float\n",
    "    df.COG = df.COG.astype('object')\n",
    "\n",
    "    # then try to split into three columns\n",
    "    try:\n",
    "        df[['COG X', 'COG Y', 'COG Z']] = df['COG'].str.split(',', expand=True)\n",
    "    except ValueError:\n",
    "        # will fail if there is nothing to split on, so create 3 cols with NaN\n",
    "        df[['COG X', 'COG Y', 'COG Z']] = np.NaN\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_zero_values(df):\n",
    "    value_cols = df.select_dtypes(exclude=[object]).columns\n",
    "    value_cols = value_cols.drop(['Level','orig_sort'])\n",
    "\n",
    "    # convert 0.0 to na\n",
    "    df[value_cols] = np.where(df[value_cols] == 0, np.NaN, df[value_cols])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_not_set_values(df):\n",
    "    df = df.replace('Not Set', np.NaN)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_na_values(df):\n",
    "    # populate Provide with 'N/A' for source code SYS and ENG\n",
    "    df['Provide'] = np.where(df['Source Code'].isin(['SYS','ENG']), 'N/A', df['Provide'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df):\n",
    "    df.loc['percent_missing'] = None\n",
    "    df.loc['percent_missing'] = df.isnull().sum(axis=0) * 100 / len(df)\n",
    "    # df.loc['percent_missing'] = df.loc['percent_missing'].astype(int)\n",
    "    df.loc['percent_missing','orig_sort'] = 'percent_missing'\n",
    "    new_df = pd.concat([df.iloc[-1:].copy(), df.iloc[:-1].copy()])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_part_no(df):\n",
    "\n",
    "    # ? means the preceding bracketed group is optional (optional s,S and trailing X)\n",
    "    # pattern = r'([A-Z]\\d{2}([s,S])?-[A-Z]\\d{4}(X)?)'\n",
    "    # pattern = r'[A-Z]\\d{2}[e]-[A-Z]\\d{5}+X?'\n",
    "    # pattern = r'([A-Z]\\d{2}[e])-([A-Z])(\\d{5})(X)'\n",
    "    pattern = r'([A-Z]\\d{2}[e])-(\\w[A-Za-z0-9]*)?-?([A-Z])(\\d{5})(X)?'\n",
    "    df[['extr_project','extr_invalid_code','extr_function','extr_pn','extr_maturity']] = df['Title'].str.extract(pattern, expand=True)\n",
    "\n",
    "    df['part_number_length'] = df['Title'].str.len()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_blanks(df):\n",
    "\n",
    "    # replace an empty string and records with only spaces\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, '%Y-%m-%d')\n",
    "    except:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_excel(df, outfile):\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # update if exists\n",
    "    try:\n",
    "        wb = load_workbook(outfile)\n",
    "        with pd.ExcelWriter(outfile, engine='openpyxl') as writer:\n",
    "            writer.workbook = wb\n",
    "            df.to_excel(writer, index=False)\n",
    "            print (\"BOM written to existing file {}\".format(outfile))\n",
    "    except (FileNotFoundError):\n",
    "        df.to_excel(outfile, index=False)\n",
    "        print (\"BOM written to new file {}\".format(outfile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XCADEmbeddedCmp means it is an Embedded Component, which means its behaves like an Assembly (so it has child parts), but it usually only exists in the context of its parent. So for your purposes, it should be flagged as an Assembly. \n",
    "\n",
    "I think KPKV5EquivalentComputed is just another element of the inertia measure function which is on the part template, so it appears whether or not its had the mass calculated. For the two parts that don't have KPKV5EquivalentComputed, it might mean they were created from a different template. \n",
    "\n",
    "‘XCAD Extension’ and ‘XCADExtension’ are the same, I think its just displaying differently on its own than when its concatenated into a string with other values. If they don't have 3DPart then they are Assemblies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subtype(df):\n",
    "    \n",
    "    df.Subtype.replace({'XCAD Extension':'Assembly',\n",
    "                        'XCADExtension':'Assembly',\n",
    "                        'XCADEmbeddedCmp':'Assembly',\n",
    "                            '3DPart':'Part3D',\n",
    "                            'XCADExposedPLMParameterSet':'COG and Mass Calculated'}, \n",
    "                            regex=True,\n",
    "                            inplace=True)\n",
    "                \n",
    "    df.Subtype = df.Subtype.str.split(',')\n",
    "    df.Subtype = df.Subtype.fillna(\"\").apply(list)\n",
    "    # dynamically create columns\n",
    "    for i in sorted(set(sum(df.Subtype.tolist(),[]))):\n",
    "        # Create a new column \n",
    "        df[i] = df.Subtype.apply(lambda x: 1 if i in x else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_child(df):\n",
    "    df['has_child'] = np.where(df.Level>=df.Level.shift(-1), 0, 1)\n",
    "    # set last row to has_child = 0 because there isn't anything below it\n",
    "    df.loc[df.index[-1],'has_child'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening /Users/mark/Downloads/ENOSCEN_APMar 12 2024 18_01_56.xlsx at: 21:01:07\n",
      "No entry in file product_structures_config.ini, section: Product_Structures, for key T48e-01-Z00003.  Using 'default'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/mtvk35311zbdwxzbj_gcnxkh0000gn/T/ipykernel_33020/23892361.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.9615384615384616' has dtype incompatible with datetime64[us], please explicitly cast to a compatible dtype first.\n",
      "  df.loc['percent_missing'] = df.isnull().sum(axis=0) * 100 / len(df)\n",
      "/var/folders/6g/mtvk35311zbdwxzbj_gcnxkh0000gn/T/ipykernel_33020/23892361.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'percent_missing' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc['percent_missing','orig_sort'] = 'percent_missing'\n",
      "/var/folders/6g/mtvk35311zbdwxzbj_gcnxkh0000gn/T/ipykernel_33020/23892361.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.9615384615384616' has dtype incompatible with datetime64[us], please explicitly cast to a compatible dtype first.\n",
      "  df.loc['percent_missing'] = df.isnull().sum(axis=0) * 100 / len(df)\n",
      "/var/folders/6g/mtvk35311zbdwxzbj_gcnxkh0000gn/T/ipykernel_33020/23892361.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'percent_missing' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc['percent_missing','orig_sort'] = 'percent_missing'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/mark/Downloads/T48e'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mwrite_to_excel\u001b[0;34m(df, outfile)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     wb \u001b[38;5;241m=\u001b[39m \u001b[43mload_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mExcelWriter(outfile, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/openpyxl/reader/excel.py:315\u001b[0m, in \u001b[0;36mload_workbook\u001b[0;34m(filename, read_only, keep_vba, data_only, keep_links)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Open the given filename and return the workbook\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m:param filename: the path to open or a file-like object\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mExcelReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_links\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/openpyxl/reader/excel.py:124\u001b[0m, in \u001b[0;36mExcelReader.__init__\u001b[0;34m(self, fn, read_only, keep_vba, data_only, keep_links)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,  fn, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_vba\u001b[38;5;241m=\u001b[39mKEEP_VBA,\n\u001b[1;32m    123\u001b[0m               data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_links\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchive \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchive\u001b[38;5;241m.\u001b[39mnamelist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/openpyxl/reader/excel.py:96\u001b[0m, in \u001b[0;36m_validate_archive\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidFileException(msg)\n\u001b[0;32m---> 96\u001b[0m archive \u001b[38;5;241m=\u001b[39m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m archive\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/zipfile/__init__.py:1323\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mark/Downloads/T48e/Updated_T48e-01-Z00003_2024-04-11.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m BOM_ordered \u001b[38;5;241m=\u001b[39m order_columns(BOM_pp)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# write out the full file\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[43mwrite_to_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBOM_ordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# write out without packaging to data shuttle\u001b[39;00m\n\u001b[1;32m    136\u001b[0m write_to_excel(BOM_ordered_without_packaging, data_shuttle_path)\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mwrite_to_excel\u001b[0;34m(df, outfile)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOM written to existing file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(outfile))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOM written to new file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(outfile))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/core/generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2335\u001b[0m     df,\n\u001b[1;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[0;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/io/formats/excel.py:946\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m     59\u001b[0m engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/io/excel/_base.py:1263\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[1;32m   1260\u001b[0m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m   1261\u001b[0m )\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/io/common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python312/lib/python3.12/site-packages/pandas/io/common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/Users/mark/Downloads/T48e'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    files = find_files(download_dir)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print (\"No files found in {}\".format(download_dir))\n",
    "       \n",
    "\n",
    "    else:\n",
    "        dict_df = {}\n",
    "\n",
    "        # loop in case more than 1 file\n",
    "        for file in files:\n",
    "            BOM = pd.DataFrame()\n",
    "            BOM = open_file(file)\n",
    "\n",
    "            time_format = \"%Y-%m-%d %H:%M\"\n",
    "            curr_time = time.strftime(time_format, time.localtime())\n",
    "\n",
    "            # check mandatory attributes are present\n",
    "            mandatory_cols = mandatory_attributes(BOM.columns)\n",
    "\n",
    "            # if 'Function Group' not in BOM.columns:\n",
    "            BOM = add_function_group(BOM)\n",
    "            BOM = rename_columns(BOM)\n",
    "\n",
    "            # read the title from first row as product to this file after\n",
    "            product = BOM['Title'].loc[0]\n",
    "\n",
    "            # populate COG x, y, z from COG field\n",
    "            # 04/04/2024 - Jannik/Carlos - don't need separate COG cols anymore\n",
    "            # BOM = COG_split(BOM)\n",
    "\n",
    "            # add an SA_Index for the add_quantities stage\n",
    "            BOM_sa = create_sa_index(BOM)\n",
    "\n",
    "            # BOM_pp = create_parent_part(BOM_sa)\n",
    "            BOM_pp = create_gparent_part(BOM_sa)\n",
    "\n",
    "            # if we've not been given quantity we need to do the roll-up ourselves\n",
    "            if 'Quantity' not in BOM.columns:\n",
    "                # this creates the quantity column\n",
    "                BOM_pp = add_quantities(BOM_pp)\n",
    "\n",
    "            # don't include 'Part Number' from 3dx - it's not the real part number and confuses Carlos' process\n",
    "            try:\n",
    "                BOM_pp.drop('Part Number', axis=1, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "            # don't keep SA_Index in output as not needed.            \n",
    "            try:\n",
    "                BOM_pp.drop('SA_Index', axis=1, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            BOM_pp = validate_part_no(BOM_pp)\n",
    "            \n",
    "            # BOM_ordered['Effectivity'] = BOM_ordered['Effectivity'].apply(convert_date)\n",
    "\n",
    "            dict_df[product] = BOM_pp\n",
    "\n",
    "            # replace only spaces in cells with NaN\n",
    "            BOM_pp = clear_blanks(BOM_pp)\n",
    "            # replace zero values with NaN\n",
    "            BOM_pp = clear_zero_values(BOM_pp)\n",
    "            # replace 'Not Set' values with NaN\n",
    "            BOM_pp = clear_not_set_values(BOM_pp)\n",
    "            # populate SYS and ENG source codes with 'N/A'\n",
    "            BOM_pp = set_na_values(BOM_pp)\n",
    "\n",
    "            # BOM_pp = split_subtype(BOM_pp)\n",
    "\n",
    "            # write out the updated filename with timestamp to the correct dir\n",
    "            project = product.split('-')[0]\n",
    "            output_file = 'Updated_{}_{}.xlsx'.format(product, curr_time.split()[0])\n",
    "            output_path = os.path.join(sharepoint_dir, project, output_file)\n",
    "            # print (\"file written to {}\".format(output_path))\n",
    "\n",
    "            # write to data shuttle directory for Carlos to pick up\n",
    "            data_shuttle_file = 'Updated_{}_{}.xlsx'.format(product, curr_time.split()[0])\n",
    "            ds_folder = data_shuttle_folder(product)\n",
    "            data_shuttle_path = os.path.join(sharepoint_dir, 'Data Shuttle', ds_folder, data_shuttle_file)\n",
    "\n",
    "            # get creation time of 3dx file \n",
    "            extract_date = datetime.datetime.fromtimestamp(Path(file).stat().st_ctime)\n",
    "            BOM_pp = add_smartsheet_cols(BOM_pp, extract_date)            \n",
    "            # format Last Export Date as datetime, with dayfirst\n",
    "            BOM_pp['Last Export Date'] = pd.to_datetime(BOM_pp['Last Export Date'], dayfirst=True)\n",
    "            # sort by orig_sort before writing out\n",
    "            BOM_pp = BOM_pp.sort_values(by='orig_sort')\n",
    "\n",
    "            BOM_pp = has_child(BOM_pp)\n",
    "\n",
    "            # drop packaging function group from the extract we send to data shuttle for processing\n",
    "            BOM_without_packaging = BOM_pp[~BOM_pp['Function Group'].str.contains('PACKAGING', na=False)]\n",
    "\n",
    "            # calculate percent missing after dropping packaging\n",
    "            BOM_without_packaging = percent_missing(BOM_without_packaging)\n",
    "            BOM_pp = percent_missing(BOM_pp)\n",
    "            BOM_without_packaging.loc['percent_missing','BOM COUNT'] = BOM_without_packaging.shape[0]\n",
    "            BOM_pp.loc['percent_missing','BOM COUNT'] = BOM_pp.shape[0]\n",
    "\n",
    "            # order the cols\n",
    "            BOM_ordered_without_packaging = order_columns(BOM_without_packaging)\n",
    "            BOM_ordered = order_columns(BOM_pp)\n",
    "            # write out the full file\n",
    "            write_to_excel(BOM_ordered, output_path)\n",
    "            # write out without packaging to data shuttle\n",
    "            write_to_excel(BOM_ordered_without_packaging, data_shuttle_path)\n",
    "            # print (\"file written to {}\".format(data_shuttle_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provide</th>\n",
       "      <th>Source Code</th>\n",
       "      <th>Source Code Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>percent_missing</th>\n",
       "      <td>93.269231</td>\n",
       "      <td>18.269231</td>\n",
       "      <td>72.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>SYS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>SYS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>N/A</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>N/A</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FAS</td>\n",
       "      <td>FAS - Fastener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BOF</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BOF</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Provide Source Code Source Code Name\n",
       "percent_missing  93.269231   18.269231        72.115385\n",
       "0                      N/A         SYS              NaN\n",
       "1                      N/A         SYS              NaN\n",
       "33                     N/A         ENG              NaN\n",
       "34                     N/A         ENG              NaN\n",
       "...                    ...         ...              ...\n",
       "25                     NaN         FAS   FAS - Fastener\n",
       "29                     NaN         NaN              NaN\n",
       "30                     NaN         NaN              NaN\n",
       "31                     NaN         BOF              NaN\n",
       "32                     NaN         BOF              NaN\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOM_pp.filter(regex=('Source|Provide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw \n",
    "\n",
    "wb = xw.Book()\n",
    "ws = wb.sheets(0)\n",
    "ws['A1'].options(pd.DataFrame, header=True, index=False).value=BOM_ordered_without_packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
